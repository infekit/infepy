{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "> This module store preprocessing funtions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from infepy.utils import read_k_file, read_csv_file, to_ls_dyna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import toml\n",
    "from infepy.utils import _merge_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def read_toml(config_file=\"../test_data/config.toml\" # Path to the config file\n",
    "):\n",
    "    \"Read setting file. The File containes the relative path to source and target.\"\n",
    "    config= toml.load(config_file)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def read_nodes(path_to_file:str # Path to file containing source template from the config file.\n",
    "               ) -> pd.DataFrame: # Numpy array of shape [n_nodes, x,y,z_displacement]\n",
    "    \"Read the nodes from the source template. The source template must be either a .key/.k file or .csv\"    \n",
    "    \n",
    "    if path_to_file.endswith('.csv') or path_to_file.endswith('.fcsv'):\n",
    "        mesh_df = read_csv_file(path_to_file)\n",
    "    elif path_to_file.endswith('.key') or path_to_file.endswith('.k'):\n",
    "        mesh_df= read_k_file(path_to_file)\n",
    "    return mesh_df\n",
    "     # if mesh_df = None, -> no file read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# read_nodes(_merge_path(config['source']['path'],config['source']['filename_mesh']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def read_landmarks(path_to_file:str # File containing Landmarks\n",
    "                   )-> pd.DataFrame: # Dataframe of length [n_landmarks]. Columns format [ID - label, x,y,z]\n",
    "    \"Read the landmarks from .csv/.key/.k file.\"\n",
    "    file_exist = False\n",
    "    if path_to_file.endswith('.csv') or path_to_file.endswith('.fcsv') :\n",
    "        landmarks_df = read_csv_file(path_to_file)\n",
    "        file_exist = True\n",
    "    elif path_to_file.endswith('.key') or path_to_file.endswith('.k') :\n",
    "        landmarks_df= read_k_file(path_to_file)\n",
    "    \n",
    "    assert not landmarks_df.empty\n",
    "    return landmarks_df\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#read_landmarks(_merge_path(config['target']['path'],config['target']['filename_landmarks']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _check_landmarks(source: pd.DataFrame, # Source dataframe\n",
    "                    target: pd.DataFrame # Target dataframe\n",
    "                    ):\n",
    "    \"This function compares the source and target Dataframe. It performs two test: if the have same amount of landmarks and if Label/IDs are in the same order.  \"\n",
    "    assert len(source) == len(target), \"Not same amount of landmarks for source and target\"\n",
    "    \n",
    "    bool = target.iloc[:,0].values == source.iloc[:,0].values\n",
    "    assert bool.any() == True, \"Order of landmarks is not the same for target and source\"\n",
    "    \n",
    "    if bool.any() == False:\n",
    "        return [i for i,x in enumerate(bool) if x == False] # => [1, 3]\n",
    "        \n",
    "    # TO DO: return values that are false. Bool gived false and extract index. Print. \n",
    "    # Sort them\n",
    "        # if they dont match, return exception -->\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# _merge_path(config['target']['path'],config['target']['filename_landmarks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# source = read_landmarks(_merge_path(config['source']['path'],config['source']['filename_landmarks']))\n",
    "# print(_merge_path(config['target']['path'],config['target']['filename_landmarks']))\n",
    "# # target = read_landmarks(_merge_path(config['target']['path'],config['target']['filename_landmarks']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def write_output(morphed_mesh : np.ndarray, # Morphed mesh\n",
    "                 morphed_file: np.ndarray, #  Path to directory to save the file\n",
    "                 mesh_file: np.ndarray, # Path to the source mesh from config file\n",
    "                 ):\n",
    "    \"Write output file for the morphed mesh in .key file format.\"    \n",
    "    if not os.path.exists(morphed_file): # write the file if doesnÂ´t exist\n",
    "        open(morphed_file, 'w').close()\n",
    "     \n",
    "    node_indicator = False\n",
    "    idx_nodes = 0\n",
    "    # Read the file and read line by line to find *node section.\n",
    "    with open(mesh_file, 'r') as fp:\n",
    "        file_content = fp.readlines()\n",
    "    for idx_line, line in enumerate(file_content):\n",
    "        if line.startswith('*NODE'):\n",
    "            node_indicator = True\n",
    "            continue\n",
    "        if node_indicator and line.startswith('*'):\n",
    "            break\n",
    "        if node_indicator:\n",
    "            line_list = list(line)\n",
    "            line_list[8:24] = to_ls_dyna(morphed_mesh[idx_nodes, 0]) # x\n",
    "            line_list[24:40] = to_ls_dyna(morphed_mesh[idx_nodes, 1]) # y\n",
    "            line_list[40:56] = to_ls_dyna(morphed_mesh[idx_nodes, 2]) # z\n",
    "            file_content[idx_line] = ''.join(line_list)\n",
    "            idx_nodes += 1\n",
    "    with open(morphed_file, 'w') as fp:\n",
    "        for line in file_content:\n",
    "            fp.write(line)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
