{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "> This module store preprocessing funtions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from infepy.utils import read_k_file, read_csv_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "logger = logging.getLogger(name='preprocessing')\n",
    "logging.basicConfig(filename=\"preprocessing.log\",filemode='w+' ,level=logging.DEBUG, force=True, format='[%(asctime)-15s] %(levelname)-8s %(filename)s %(funcName)s line %(lineno)d %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import toml\n",
    "from infepy.utils import _merge_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def read_toml(  config_file= \"Config.toml\" # Path to the config file\n",
    "):\n",
    "    \"Read setting file. The File containes the relative path to source and target.\"\n",
    "    config= toml.load(config_file)\n",
    "    return config\n",
    "config  = read_toml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def read_nodes(path_to_file:str # path to file containing source template\n",
    "               ) -> pd.DataFrame: # Numpy array of shape [n_nodes, x,y,z_displacement]\n",
    "    \"Read the nodes from the source template. The source template must be either a .key/.k file or .csv\"    \n",
    "    try:\n",
    "        if path_to_file.endswith('.csv'):\n",
    "            mesh_df = read_csv_file(path_to_file)\n",
    "        elif path_to_file.endswith('.key') or path_to_file.endswith('.k'):\n",
    "            mesh_df= read_k_file(path_to_file)\n",
    "        return mesh_df\n",
    "    except:\n",
    "        logger.exception(\"Read_Nodes: No readable files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label - node id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>591310</td>\n",
       "      <td>-181.550415</td>\n",
       "      <td>-145.430511</td>\n",
       "      <td>404.375488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3591330</td>\n",
       "      <td>-43.251625</td>\n",
       "      <td>-196.187363</td>\n",
       "      <td>194.874634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3591332</td>\n",
       "      <td>-37.076435</td>\n",
       "      <td>-219.493759</td>\n",
       "      <td>205.082413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label - node id           x           y           z\n",
       "0           591310 -181.550415 -145.430511  404.375488\n",
       "1          3591330  -43.251625 -196.187363  194.874634\n",
       "2          3591332  -37.076435 -219.493759  205.082413"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_nodes(_merge_path(config['source']['path'],config['source']['filename_mesh']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def read_landmarks(path_to_file:str # .csv or .key file containing Landmarks\n",
    "                   )-> pd.DataFrame: # Dataframe of length [n_landmarks] divided in columns [ID - label, x,y,z]\n",
    "    \"Read the landmarks from .csv/.key/.k file.\"\n",
    "    file_exist = False\n",
    "    if path_to_file.endswith('.csv'):\n",
    "        landmarks_df = read_csv_file(path_to_file)\n",
    "        file_exist = True\n",
    "    elif path_to_file.endswith('.key') or path_to_file.endswith('.k') :\n",
    "        landmarks_df= read_k_file(path_to_file)\n",
    "    \n",
    "    assert not landmarks_df.empty\n",
    "    return landmarks_df\n",
    "# except:\n",
    "    #     logger.exception(\"Read_Landamarks - No readable files\")\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label - node id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>100.001907</td>\n",
       "      <td>110.13913</td>\n",
       "      <td>-100.980621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label - node id           x          y           z\n",
       "0                1  100.001907  110.13913 -100.980621"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_landmarks(_merge_path(config['source']['path'],config['source']['filename_landmarks']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _check_landmarks(source: pd.DataFrame, # Source dataframe\n",
    "                    target: pd.DataFrame # Target dataframe\n",
    "                    ):\n",
    "    \"This function compares the source and target Dataframe. It performs two test: if the have same amount of landmarks and if Label/IDs are in the same order.  \"\n",
    "    try:\n",
    "        assert len(source) == len(target), \"Not same amount of landmarks for source and target\"\n",
    "        \n",
    "        bool = target.iloc[:,0].values == source.iloc[:,0].values\n",
    "        assert bool.any() == True, \"Order of landmarks is not the same for target and source\"\n",
    "        \n",
    "        if bool.any() == False:\n",
    "            return [i for i,x in enumerate(bool) if x==False] # => [1, 3]\n",
    "            \n",
    "        # TO DO: return values that are false. Bool gived false and extract index. Print. \n",
    "        # Sort them\n",
    "        # if they dont match, return exception -->\n",
    "    except:\n",
    "        logger.exception(\"Invalid Landmarks. Need to be same size and order\")       \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/target/landmarks_target.key'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_merge_path(config['target']['path'],config['target']['filename_landmarks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m source \u001b[39m=\u001b[39m read_landmarks(_merge_path(config[\u001b[39m'\u001b[39m\u001b[39msource\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m'\u001b[39m],config[\u001b[39m'\u001b[39m\u001b[39msource\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mfilename_landmarks\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[0;32m----> 2\u001b[0m target \u001b[39m=\u001b[39m read_landmarks(_merge_path(config[\u001b[39m'\u001b[39;49m\u001b[39mtarget\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mpath\u001b[39;49m\u001b[39m'\u001b[39;49m],config[\u001b[39m'\u001b[39;49m\u001b[39mtarget\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mfilename_landmarks\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n",
      "Cell \u001b[0;32mIn [10], line 10\u001b[0m, in \u001b[0;36mread_landmarks\u001b[0;34m(path_to_file)\u001b[0m\n\u001b[1;32m      8\u001b[0m     file_exist \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39melif\u001b[39;00m path_to_file\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.key\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m path_to_file\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.k\u001b[39m\u001b[39m'\u001b[39m) :\n\u001b[0;32m---> 10\u001b[0m     landmarks_df\u001b[39m=\u001b[39m read_k_file(path_to_file)\n\u001b[1;32m     12\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m landmarks_df\u001b[39m.\u001b[39mempty\n\u001b[1;32m     13\u001b[0m \u001b[39mreturn\u001b[39;00m landmarks_df\n",
      "File \u001b[0;32m/mnt/c/Users/chiarar/OneDrive - Chalmers/SSMT - Subject Specific Modeling Toolstack/Github/infepy/infepy/utils.py:47\u001b[0m, in \u001b[0;36mread_k_file\u001b[0;34m(path_to_file)\u001b[0m\n\u001b[1;32m     44\u001b[0m node_coords \u001b[39m=\u001b[39m []\n\u001b[1;32m     45\u001b[0m find_nodes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[39massert\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(path_to_file)\n\u001b[1;32m     49\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path_to_file, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m fp:\n\u001b[1;32m     50\u001b[0m     \u001b[39mfor\u001b[39;00m _, line \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(fp):\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# source = read_landmarks(_merge_path(config['source']['path'],config['source']['filename_landmarks']))\n",
    "# target = read_landmarks(_merge_path(config['target']['path'],config['target']['filename_landmarks']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def write_output(morphed_mesh : np.ndarray, # Morphed mesh\n",
    "                 morphed_file: np.ndarray, #  path to directory to save the file\n",
    "                 mesh_file: np.ndarray, # path to the source mesh from read_setting()\n",
    "                 ):\n",
    "    \"Write an output file for the morphed mesh in key file format.\"\n",
    "    assert os.path.exists(morphed_file)\n",
    "    \n",
    "    if not os.path.exists(morphed_file): # write the file if doesn´t exist\n",
    "        open(morphed_file, 'w').close()\n",
    "     \n",
    "    node_indicator = False\n",
    "    idx_nodes = 0\n",
    "    # Read the file and read line by line to find *node section.\n",
    "    with open(mesh_file, 'r') as fp:\n",
    "        file_content = fp.readlines()\n",
    "    for idx_line, line in enumerate(file_content):\n",
    "        if line.startswith('*NODE'):\n",
    "            node_indicator = True\n",
    "            continue\n",
    "        if node_indicator and line.startswith('*'):\n",
    "            break\n",
    "        if node_indicator:\n",
    "            line_list = list(line)\n",
    "            line_list[8:24] = to_ls_dyna(morphed_mesh[idx_nodes, 0]) # x\n",
    "            line_list[24:40] = to_ls_dyna(morphed_mesh[idx_nodes, 1]) # y\n",
    "            line_list[40:56] = to_ls_dyna(morphed_mesh[idx_nodes, 2]) # z\n",
    "            file_content[idx_line] = ''.join(line_list)\n",
    "            idx_nodes += 1\n",
    "    with open(morphed_file, 'w') as fp:\n",
    "        for line in file_content:\n",
    "            fp.write(line)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('infekit')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
